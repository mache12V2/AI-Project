{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1hUsfYDdP-H",
        "outputId": "f2f703cb-d26e-4013-eaf8-7d0763eef01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorflow[and-cuda] in /home/mache12/.local/lib/python3.10/site-packages (2.13.0)\n",
            "\u001b[33mWARNING: tensorflow 2.13.0 does not provide the extra 'and-cuda'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.13.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.6.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.0.0)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.24.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (16.0.6)\n",
            "Requirement already satisfied: packaging in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow[and-cuda]) (59.6.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.58.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.34.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.13.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow[and-cuda]) (1.16.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (23.5.26)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.9.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.13.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (0.7.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (3.4.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (2.3.6)\n",
            "Requirement already satisfied: urllib3<2.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (1.26.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mache12/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mache12/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mache12/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mache12/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mache12/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/mache12/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow[and-cuda]) (3.2.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tensorflow in /home/mache12/.local/lib/python3.10/site-packages (2.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (1.58.0)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (2.0.0)\n",
            "Requirement already satisfied: packaging in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/mache12/.local/lib/python3.10/site-packages (from tensorflow) (1.24.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.25.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /home/mache12/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: urllib3<2.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/mache12/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /home/mache12/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/mache12/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/mache12/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/mache12/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/mache12/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "# For GPU users\n",
        "!pip install tensorflow[and-cuda]\n",
        "# For CPU users\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYARs0fOliEL",
        "outputId": "ec0de60e-d8e0-43b8-f896-e88f02edd437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: opencv-python in /home/mache12/.local/lib/python3.10/site-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /home/mache12/.local/lib/python3.10/site-packages (from opencv-python) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PjdNJDAlpWq",
        "outputId": "01d46f54-a362-4371-8dcc-e8b734dd617d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scikit-learn in /home/mache12/.local/lib/python3.10/site-packages (1.3.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/mache12/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/lib/python3/dist-packages (from scikit-learn) (1.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /home/mache12/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mache12/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M75MuDdclZT2",
        "outputId": "a9e48c8b-8fc3-4e7b-c8ae-a34b8b076891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: mediapipe in /home/mache12/.local/lib/python3.10/site-packages (0.10.7)\n",
            "Requirement already satisfied: numpy in /home/mache12/.local/lib/python3.10/site-packages (from mediapipe) (1.24.3)\n",
            "Requirement already satisfied: absl-py in /home/mache12/.local/lib/python3.10/site-packages (from mediapipe) (2.0.0)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /home/mache12/.local/lib/python3.10/site-packages (from mediapipe) (0.4.6)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /home/mache12/.local/lib/python3.10/site-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: opencv-contrib-python in /home/mache12/.local/lib/python3.10/site-packages (from mediapipe) (4.8.1.78)\n",
            "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from mediapipe) (3.5.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/lib/python3/dist-packages (from mediapipe) (21.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /home/mache12/.local/lib/python3.10/site-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: CFFI>=1.0 in /home/mache12/.local/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /home/mache12/.local/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NpFUvvLLkZD5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-11-07 18:05:56.950062: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-11-07 18:05:56.954407: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-07 18:05:57.046000: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2023-11-07 18:05:57.046958: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-11-07 18:05:58.813671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import mediapipe as mp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "mp_holistic  = mp.solutions.holistic #holistic model\n",
        "mp_drawing = mp.solutions.drawing_utils #utilidades de dibujo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mediapipe_detection(image,model):\n",
        "    image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB) # convierte de BGR a RGB\n",
        "    image.flags.writeable = False  #La imagen no es writeable\n",
        "    results = model.process(image)  #image es el frame del open cv, hace la prediccion\n",
        "    image.flags.writeable = True     #la imagen es writable \n",
        "    image = cv2.cvtColor(image,cv2.COLOR_RGB2BGR) #convierte de RGB a BGR\n",
        "    return image,results \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mediapipe_detection(image, model):\n",
        "    # Convert from BGR to RGB\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Make a copy of the image to avoid modifying the original image\n",
        "    input_image = np.copy(image_rgb)\n",
        "    \n",
        "    # Make the input image non-writable before processing\n",
        "    input_image.flags.writeable = False\n",
        "    \n",
        "    # Perform detection\n",
        "    results = model.process(input_image)\n",
        "    \n",
        "    # Make the input image writable again\n",
        "    input_image.flags.writeable = True\n",
        "    \n",
        "    # Convert the image back from RGB to BGR\n",
        "    image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
        "    \n",
        "    return image, results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_landmarks(image,results): #dibujar las marcas en la imagen\n",
        "    mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_TESSELATION) #dibuja las conecciones de la cara\n",
        "    mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS) #Dibuja las pose conecciones\n",
        "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS) #dibuja las conecciones de la mano izquierda\n",
        "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS) #dibuja las conecciones de la mano derecha \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def draw_styled_landmarks(image,results): #dibuja los landmarks refinados \n",
        "    mp_drawing.draw_landmarks(image,results.face_landmarks,mp_holistic.FACEMESH_TESSELATION,mp_drawing.DrawingSpec(color=(50,110,10),thickness=1,circle_radius=1),mp_drawing.DrawingSpec(color=(80,256,121),thickness=1,circle_radius=1)) #dibuja las conecciones de la cara\n",
        "    mp_drawing.draw_landmarks(image,results.pose_landmarks,mp_holistic.POSE_CONNECTIONS,mp_drawing.DrawingSpec(color=(80,22,10),thickness=2,circle_radius=4),mp_drawing.DrawingSpec(color=(80,44,121),thickness=2,circle_radius=2)) #Dibuja las pose conecciones\n",
        "    mp_drawing.draw_landmarks(image,results.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS,mp_drawing.DrawingSpec(color=(80,22,76),thickness=2,circle_radius=4),mp_drawing.DrawingSpec(color=(121,44,250),thickness=2,circle_radius=2)) #dibuja las conecciones de la mano izquierda\n",
        "    mp_drawing.draw_landmarks(image,results.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS,mp_drawing.DrawingSpec(color=(254,117,66),thickness=2,circle_radius=4),mp_drawing.DrawingSpec(color=(245,66,230),thickness=2,circle_radius=2)) #dibuja las conecciones de la mano derecha \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tIaDAFRvmRdi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/mache12/.local/lib/python3.10/site-packages/cv2/qt/plugins\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
            "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n"
          ]
        }
      ],
      "source": [
        "#abre la webcam y va entre los frames\n",
        "cap = cv2.VideoCapture(0)\n",
        "#setteamos el modelo de mediapipe\n",
        "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
        "  while cap.isOpened():\n",
        "    #lee el feed\n",
        "    ret,frame = cap.read()\n",
        "\n",
        "    #hace la deteccion\n",
        "    image, results = mediapipe_detection(frame,holistic)\n",
        "    print(results)\n",
        "    #dibuja  los landmarks\n",
        "    draw_styled_landmarks(image,results)\n",
        "    #Lo muestra en la pantalla\n",
        "    cv2.imshow('OpenCV Feed',image)\n",
        "\n",
        "    #Rompe el loop si se apreta q\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "      break\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "pose = [] #array para gaursar los landmarks\n",
        "for res in results.pose_landmarks.landmark:\n",
        "    test = np.array([res.x,res.y,res.z,res.visibility])\n",
        "    pose.append(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "pose = np.array([[res.x,res.y,res.z,res.visibility]for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)#todos los landmarks en un solo array grande\n",
        "face = np.array([[res.x,res.y,res.z]for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)#todos los landmarks en un solo array grande\n",
        "lh = np.array([[res.x,res.y,res.z]for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)#todos los landmarks en un solo array grande\n",
        "rh = np.array([[res.x,res.y,res.z]for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)#todos los landmarks en un solo array grande"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def extract_keypoints(results):\n",
        "    pose = np.array([[res.x,res.y,res.z,res.visibility]for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)#todos los landmarks en un solo array grande\n",
        "    face = np.array([[res.x,res.y,res.z]for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)#todos los landmarks en un solo array grande\n",
        "    lh = np.array([[res.x,res.y,res.z]for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)#todos los landmarks en un solo array grande\n",
        "    rh = np.array([[res.x,res.y,res.z]for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)#todos los landmarks en un solo array grande\n",
        "    return np.concatenate([pose,face,lh,rh])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_test = extract_keypoints(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.29774281,  0.53543627, -1.83118761, ...,  0.        ,\n",
              "        0.        ,  0.        ])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save('0',result_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0.29774281,  0.53543627, -1.83118761, ...,  0.        ,\n",
              "        0.        ,  0.        ])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.load('0.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1662,)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "extract_keypoints(results).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "#path para la data exportada\n",
        "DATA_PATH = os.path.join('MP_Data') # variable que guarda \n",
        "\n",
        "#Aciones detectadas\n",
        "actions = np.array(['hello','thanks','iloveyou'])\n",
        "no_sequences = 30 #Cantidad de videos que se recolectaran por accion\n",
        "sequence_length = 40 #30 es la cantidad de frames de data que va a utilizar \n",
        "#lo que se hara para la info es recolectar 30 videos de 30 frames cada un por accion (son 3 acciones) por la cantidad de keypoints que se tienen que son 1662 (entre todas las  variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "for action in actions: \n",
        "    for sequence in range(no_sequences):\n",
        "        try: \n",
        "            os.makedirs(os.path.join(DATA_PATH,action,str(sequence)))\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(0)\n",
        "#setteamos el modelo de mediapipe\n",
        "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
        "    for action in actions:\n",
        "        for sequence in range(no_sequences):\n",
        "            for frame_num in range(sequence_length):\n",
        "    #lee el feed\n",
        "                ret,frame = cap.read()\n",
        "\n",
        "    #hace la deteccion\n",
        "                image, results = mediapipe_detection(frame,holistic)\n",
        "                #print(results)\n",
        "            #dibuja  los landmarks\n",
        "                draw_styled_landmarks(image,results)\n",
        "            \n",
        "                if frame_num == 0 :\n",
        "                    cv2.putText(image,'Sarting Collection',(120,200),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2,cv2.LINE_AA)# IMprime Starting Collection\n",
        "                    cv2.putText(image,'Colelecting Frames for {} Video Number {}'.format(action,sequence),(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
        "                    cv2.imshow('OpenCV Feed',image)\n",
        "                    cv2.waitKey(500)\n",
        "            \n",
        "                else :\n",
        "                    cv2.putText(image,'Colelecting Frames for {} Video Number {}'.format(action,sequence),(15,12),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,0,255),1,cv2.LINE_AA)\n",
        "                    cv2.imshow('OpenCV Feed',image)\n",
        "\n",
        "                #Exportamos los Keypoints\n",
        "                keypoints = extract_keypoints(results)\n",
        "                npy_path = os.path.join(DATA_PATH,action,str(sequence),str(frame_num))\n",
        "                np.save(npy_path,keypoints)\n",
        "\n",
        "                #Rompe el loop si se apreta q\n",
        "            if cv2.waitKey(5) & 0xFF == ord('q'):\n",
        "                break\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_map = {label:num for num, label in enumerate(actions)} #esto loopea entre cada una de las palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_map #se estructura en un array grande por accion, se termina creando 90 arrays con 30 fraames en cada array con 1662 valores que representan los keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequences, labels = [],[] # sequences representa la dara y labels representa los label\n",
        "for action in actions: # se loopea entre las acciones y las sequencias\n",
        "    for sequence in range(no_sequences):\n",
        "        window = [] #windows representa todas las ventanas de video\n",
        "        for frame_num in range(sequence_length):\n",
        "            res = np.load(os.path.join(DATA_PATH,action,str(sequence),\"{}.npy\".format(frame_num))) # se cargan los diferentes frames desde las carpetas  \n",
        "            window.append(res)\n",
        "        sequences.append(window)\n",
        "        labels.append(label_map[action])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sequences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = np.array(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = to_categorical(labels).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.callbacks import TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_dir = os.path.join('Logs')\n",
        "tb_callback = TensorBoard(log_dir = log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model = Sequential() # Se rean layers para entrenar los modelos \n",
        "model.add(LSTM(64,return_sequences = True, activation='relu',input_shape = (30,1662)))\n",
        "model.add(LSTM(128,return_sequences = True, activation='relu'))\n",
        "model.add(LSTM(64,return_sequences = False, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(actions.shape[0],activation = 'softmax')) #softmas te da un conjunto de probabilidades que sumadas te suman 1  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions[np.argmax(res)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.fit(x_train,y_train, epochs = 2000, callbacks = [tb_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "res = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions[np.sum(res[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions[np.argmax(y_test[0])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save('action.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = model.predict(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ytrue = np.argmax(y_test, axis = 1).tolist()\n",
        "yhat =  np.argmax(yhat,axis = 1).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "multilabel_confusion_matrix(ytrue,yhat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy_score(ytrue,yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*Real time Testing*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = [(245,117,16),(117,245,16),(16,117,245)]\n",
        "def prob_viz(res,actions,imput_frame,colors):\n",
        "    output_frame = imput_frame.copy()\n",
        "    for num,prob in enumerate(res):\n",
        "        cv2.rectangle(output_frame,(0,60+num*40),(int(prob*100),90+num*40), colors[num,-1])\n",
        "        cv2.putText(output_frame,actions[num],(0,85+num*40), cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
        "    \n",
        "    return output_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18,18))\n",
        "plt.imshow(prob_viz(res,actions,image,colors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Variables de deteccion\n",
        "sequence = []\n",
        "sentence = []\n",
        "predictions = []\n",
        "threshold = 0.5\n",
        "\n",
        "\n",
        "#abre la webcam y va entre los frames\n",
        "cap = cv2.VideoCapture(0)\n",
        "#setteamos el modelo de mediapipe\n",
        "with mp_holistic.Holistic(min_detection_confidence = 0.5, min_tracking_confidence = 0.5) as holistic:\n",
        "  while cap.isOpened():\n",
        "    #lee el feed\n",
        "    ret,frame = cap.read()\n",
        "\n",
        "    #hace la deteccion\n",
        "    image, results = mediapipe_detection(frame,holistic)\n",
        "    print(results)\n",
        "    #dibuja  los landmarks\n",
        "    draw_styled_landmarks(image,results)\n",
        "    keypoints = extract_keypoints(results)\n",
        "    sequence.append(keypoints)\n",
        "    #[-30:] son los ultimos 30 frames/ 30 set de keypoints\n",
        "    sequence = sequence[-30:]\n",
        "    \n",
        "    #si la longitud de la sequencia tiene una longitud de 30 frames, recien ahi se hace la prediccion\n",
        "    if len(sequence) == 30:\n",
        "        res = model.predict(np.expand_dims(sequence,axis = 0))[0]\n",
        "        print(actions[np.argmax(res)])\n",
        "        predictions.append(np.argmax(res))\n",
        "        \n",
        "        \n",
        "       #viz Logic \n",
        "        if np.unique(predictions[-10:])[0] == np.argmax(res):\n",
        "          if res[np.argmax(res)] > threshold:\n",
        "            if len(sentence) > 0:\n",
        "              if actions[np.argmax(res)] != sentence[-1]:\n",
        "                sentence.append(actions[np.argmax(res)])\n",
        "              else:\n",
        "                sentence.append(actions[np.argmax(res)])\n",
        "      \n",
        "          if len(sentence) > 5:\n",
        "            sentence = sentence[-5:]\n",
        "    \n",
        "    #Visualizar probabilidades\n",
        "    image = prob_viz(res,actions,image, colors)\n",
        "    \n",
        "    cv2.rectangle(image,(0,0),(640,40),(245,117,16),-1)\n",
        "    cv2.putText(image,' '.join(sentence),(3,30),cv2.FONT_HERSHEY_SIMPLEX,1,(255,255,255),2,cv2.LINE_AA)\n",
        "          \n",
        "    \n",
        "    #Lo muestra en la pantalla\n",
        "    cv2.imshow('OpenCV Feed',image)\n",
        "\n",
        "    #Rompe el loop si se apreta q\n",
        "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
        "      break\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.predict(x_test[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
